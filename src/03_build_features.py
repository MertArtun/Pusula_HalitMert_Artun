#!/usr/bin/env python3
"""
Fiziksel TÄ±p & Rehabilitasyon Veri Analizi - Ã–zellik MÃ¼hendisliÄŸi

Bu script temizlenmiÅŸ veriyi alÄ±r ve model iÃ§in hazÄ±r Ã¶zellik matrisi oluÅŸturur.
One-Hot Encoding, Ã§oklu deÄŸerli alanlar iÃ§in binary Ã¶zellikler ve 
standardizasyon uygular.

KullanÄ±m:
    python src/03_build_features.py --input-csv data/processed/clean_minimal.csv --top_k 50
"""

import argparse
import os
import sys
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack

# utils modÃ¼lÃ¼nÃ¼ import et
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from utils import (
    create_multilabel_features,
    ensure_directory_exists,
    normalize_text_token,
    split_list
)
from common_logging import features_logger as logger


def load_data(csv_path: str) -> pd.DataFrame:
    """
    CSV dosyasÄ±ndan veriyi yÃ¼kler.
    
    Args:
        csv_path (str): CSV dosya yolu
        
    Returns:
        pd.DataFrame: YÃ¼klenen veri
    """
    try:
        df = pd.read_csv(csv_path)
        logger.info(f"âœ… Veri baÅŸarÄ±yla yÃ¼klendi: {df.shape[0]} satÄ±r, {df.shape[1]} sÃ¼tun")
        return df
    except FileNotFoundError:
        logger.error(f"âŒ Hata: CSV dosyasÄ± bulunamadÄ±: {csv_path}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"âŒ Hata: CSV dosyasÄ± okunurken hata oluÅŸtu: {e}")
        sys.exit(1)


def collapse_rare(series: pd.Series, min_freq: int = 10, other_label: str = "Diger") -> pd.Series:
    """
    Nadir kategorileri belirtilen etiket altÄ±nda birleÅŸtirir.
    
    Args:
        series (pd.Series): Kategorik seri
        min_freq (int): Minimum frekans eÅŸiÄŸi
        other_label (str): Nadir kategoriler iÃ§in etiket
        
    Returns:
        pd.Series: Nadir kategoriler birleÅŸtirilmiÅŸ seri
    """
    vc = series.value_counts(dropna=False)
    keep = set(vc[vc >= min_freq].index)
    return series.apply(lambda x: x if x in keep else other_label)


def join_tokens(series: pd.Series) -> list:
    """
    Ã‡oklu deÄŸerli seriyi normalize edilmiÅŸ token'larla birleÅŸtirir.
    
    Args:
        series (pd.Series): Ã‡oklu deÄŸerli seri
        
    Returns:
        list: Her satÄ±r iÃ§in normalize edilmiÅŸ token'lar birleÅŸtirilmiÅŸ string listesi
    """
    docs = []
    for v in series.fillna(""):
        toks = [normalize_text_token(t) for t in split_list(v)]
        docs.append(" ".join(toks))
    return docs


def create_multilabel_features_all(df: pd.DataFrame, top_k: int = 50) -> tuple:
    """
    TÃ¼m Ã§oklu deÄŸerli alanlar iÃ§in binary Ã¶zellikler oluÅŸturur.
    
    Args:
        df (pd.DataFrame): Veri Ã§erÃ§evesi
        top_k (int): Her alan iÃ§in en sÄ±k gÃ¶rÃ¼len kaÃ§ Ã¶ÄŸe
        
    Returns:
        tuple: (Ã¶zellik_eklenmis_df, yeni_kolon_listesi)
    """
    print(f"ğŸ”§ Ã‡oklu deÄŸerli alanlar iÃ§in top-{top_k} binary Ã¶zellikler oluÅŸturuluyor...")
    
    df_features = df.copy()
    all_new_columns = []
    
    multi_value_columns = ['KronikHastalik', 'Alerji', 'Tanilar', 'UygulamaYerleri']
    existing_multi_cols = [col for col in multi_value_columns if col in df.columns]
    
    for col in existing_multi_cols:
        print(f"   ğŸ“‹ {col} iÃ§in binary Ã¶zellikler oluÅŸturuluyor...")
        df_features, new_cols = create_multilabel_features(
            df_features, col, top_k=top_k, prefix="ML"
        )
        all_new_columns.extend(new_cols)
        print(f"      âœ… {len(new_cols)} binary Ã¶zellik eklendi")
    
    print(f"âœ… Toplam {len(all_new_columns)} binary Ã¶zellik oluÅŸturuldu")
    return df_features, all_new_columns


def prepare_feature_columns(df: pd.DataFrame, ml_columns: list) -> dict:
    """
    FarklÄ± veri tiplerindeki sÃ¼tunlarÄ± kategorilere ayÄ±rÄ±r.
    
    Args:
        df (pd.DataFrame): Veri Ã§erÃ§evesi
        ml_columns (list): ML binary sÃ¼tunlarÄ±
        
    Returns:
        dict: SÃ¼tun kategorileri
    """
    print("ğŸ“Š Ã–zellik sÃ¼tunlarÄ± kategorize ediliyor...")
    
    # Hedef deÄŸiÅŸken ve ID sÃ¼tunlarÄ±
    target_column = 'TedaviSuresi_num'
    id_columns = ['HastaNo']
    
    # Kategorik sÃ¼tunlar (One-Hot encoding iÃ§in)
    categorical_columns = [
        'Cinsiyet', 'KanGrubu', 'Uyruk', 'Bolum', 'TedaviAdi'
    ]
    existing_categorical = [col for col in categorical_columns if col in df.columns]
    
    # SayÄ±sal sÃ¼tunlar (standardizasyon iÃ§in)
    numeric_columns = []
    for col in df.select_dtypes(include=[np.number]).columns:
        if (col not in id_columns and 
            col != target_column and 
            not col.endswith('_sayisi') and
            col not in ml_columns):
            numeric_columns.append(col)
    
    # SayÄ± sÃ¼tunlarÄ± (Ã§oklu deÄŸerli alanlarÄ±n sayÄ±larÄ±)
    count_columns = [col for col in df.columns if col.endswith('_sayisi')]
    
    # Atlanacak sÃ¼tunlar
    skip_columns = (id_columns + [target_column] + 
                   ['TedaviSuresi', 'UygulamaSuresi'] +  # Orijinal string sÃ¼tunlarÄ±
                   ['KronikHastalik', 'Alerji', 'Tanilar', 'UygulamaYerleri'])  # Ã‡oklu deÄŸerli orijinal sÃ¼tunlar
    
    column_info = {
        'target': target_column,
        'id': id_columns,
        'categorical': existing_categorical,
        'numeric': numeric_columns,
        'count': count_columns,
        'ml_binary': ml_columns,
        'skip': skip_columns
    }
    
    print(f"   ğŸ¯ Hedef: {target_column}")
    print(f"   ğŸ·ï¸  Kategorik: {len(existing_categorical)} sÃ¼tun")
    print(f"   ğŸ”¢ SayÄ±sal: {len(numeric_columns)} sÃ¼tun")
    print(f"   ğŸ“Š SayÄ±: {len(count_columns)} sÃ¼tun")
    print(f"   ğŸ¤– ML Binary: {len(ml_columns)} sÃ¼tun")
    print(f"   â­ï¸  Atlanan: {len(skip_columns)} sÃ¼tun")
    
    return column_info


def create_feature_pipeline(column_info: dict) -> ColumnTransformer:
    """
    Ã–zellik iÅŸleme pipeline'Ä± oluÅŸturur.
    
    Args:
        column_info (dict): SÃ¼tun bilgileri
        
    Returns:
        ColumnTransformer: Ã–zellik iÅŸleme pipeline'Ä±
    """
    print("ğŸ”§ Ã–zellik iÅŸleme pipeline'Ä± oluÅŸturuluyor...")
    
    transformers = []
    
    # SayÄ±sal sÃ¼tunlar iÃ§in: median imputation + standardization
    if column_info['numeric']:
        numeric_pipeline = Pipeline([
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ])
        transformers.append(('numeric', numeric_pipeline, column_info['numeric']))
        print(f"   ğŸ”¢ SayÄ±sal pipeline: median imputation + standardization")
    
    # SayÄ± sÃ¼tunlarÄ± iÃ§in: 0 ile doldur + standardization
    if column_info['count']:
        count_pipeline = Pipeline([
            ('imputer', SimpleImputer(strategy='constant', fill_value=0)),
            ('scaler', StandardScaler())
        ])
        transformers.append(('count', count_pipeline, column_info['count']))
        print(f"   ğŸ“Š SayÄ± pipeline: 0 ile doldur + standardization")
    
    # Kategorik sÃ¼tunlar iÃ§in: most frequent + one-hot encoding
    if column_info['categorical']:
        categorical_pipeline = Pipeline([
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))
        ])
        transformers.append(('categorical', categorical_pipeline, column_info['categorical']))
        print(f"   ğŸ·ï¸  Kategorik pipeline: most frequent + one-hot encoding")
    
    # ML binary sÃ¼tunlarÄ± iÃ§in: hiÃ§bir iÅŸlem yapmaz (zaten 0/1)
    if column_info['ml_binary']:
        from sklearn.preprocessing import FunctionTransformer
        ml_pipeline = FunctionTransformer(validate=False)
        transformers.append(('ml_binary', ml_pipeline, column_info['ml_binary']))
        print(f"   ğŸ¤– ML Binary: hiÃ§bir iÅŸlem uygulanmaz")
    
    # ColumnTransformer oluÅŸtur
    preprocessor = ColumnTransformer(
        transformers=transformers,
        remainder='drop'  # DiÄŸer sÃ¼tunlarÄ± at
    )
    
    return preprocessor


def get_feature_names(preprocessor: ColumnTransformer, column_info: dict) -> list:
    """
    Ä°ÅŸlenmiÅŸ Ã¶zellik isimlerini oluÅŸturur.
    
    Args:
        preprocessor (ColumnTransformer): Fit edilmiÅŸ preprocessor
        column_info (dict): SÃ¼tun bilgileri
        
    Returns:
        list: Ã–zellik isimleri
    """
    feature_names = []
    
    # SayÄ±sal sÃ¼tun isimleri
    if column_info['numeric']:
        feature_names.extend([f"num_{col}" for col in column_info['numeric']])
    
    # SayÄ± sÃ¼tun isimleri
    if column_info['count']:
        feature_names.extend([f"count_{col}" for col in column_info['count']])
    
    # Kategorik sÃ¼tun isimleri (One-Hot encoding sonrasÄ±)
    if column_info['categorical']:
        # One-Hot encoder'dan Ã¶zellik isimlerini al
        categorical_transformer = preprocessor.named_transformers_['categorical']
        onehot_encoder = categorical_transformer.named_steps['onehot']
        
        cat_feature_names = []
        for i, col in enumerate(column_info['categorical']):
            categories = onehot_encoder.categories_[i]
            # drop='first' kullandÄ±ÄŸÄ±mÄ±z iÃ§in ilk kategoriyi atla
            for cat in categories[1:]:
                cat_feature_names.append(f"cat_{col}_{cat}")
        
        feature_names.extend(cat_feature_names)
    
    # ML binary sÃ¼tun isimleri
    if column_info['ml_binary']:
        feature_names.extend(column_info['ml_binary'])
    
    return feature_names


def build_features(df: pd.DataFrame, top_k: int = 50, min_freq: int = 10, text_mode: str = "mlbin") -> tuple:
    """
    Ã–zellik matrisini oluÅŸturur.
    
    Args:
        df (pd.DataFrame): TemizlenmiÅŸ veri
        top_k (int): Ã‡oklu deÄŸerli alanlar iÃ§in top-K
        min_freq (int): Nadir kategoriler iÃ§in minimum frekans
        text_mode (str): "mlbin" veya "tfidf"
        
    Returns:
        tuple: (X, y, feature_names, column_info)
    """
    print("ğŸ—ï¸  Ã–zellik matrisi oluÅŸturuluyor...")
    
    df_processed = df.copy()
    
    # Nadir kategorileri birleÅŸtir
    print(f"ğŸ”§ Nadir kategoriler birleÅŸtiriliyor (min_freq={min_freq})...")
    base_cat = ["Cinsiyet", "KanGrubu", "Uyruk", "Bolum", "TedaviAdi"]
    for c in base_cat:
        if c in df_processed.columns:
            original_unique = df_processed[c].nunique()
            df_processed[c] = collapse_rare(df_processed[c], min_freq=min_freq)
            new_unique = df_processed[c].nunique()
            print(f"   ğŸ“Š {c}: {original_unique} â†’ {new_unique} kategori")
    
    # Text mode'a gÃ¶re Ã¶zellik Ã§Ä±karma
    if text_mode == "mlbin":
        print("ğŸ”§ Multi-label binary Ã¶zellikler oluÅŸturuluyor...")
        # Ã‡oklu deÄŸerli alanlar iÃ§in binary Ã¶zellikler oluÅŸtur (normalize edilmiÅŸ)
        df_with_ml, ml_columns = create_multilabel_features_all(df_processed, top_k)
        
        # SÃ¼tunlarÄ± kategorize et
        column_info = prepare_feature_columns(df_with_ml, ml_columns)
        
        # TF-IDF kullanmayacaÄŸÄ±z
        tfidf_features = None
        tfidf_feature_names = []
        
    elif text_mode == "tfidf":
        print("ğŸ”§ TF-IDF Ã¶zellikleri oluÅŸturuluyor (sadece TanÄ±lar iÃ§in)...")
        # Sadece diÄŸer Ã§oklu deÄŸerli alanlar iÃ§in ML binary
        multi_cols_except_tanilar = ['KronikHastalik', 'Alerji', 'UygulamaYerleri']
        df_with_ml = df_processed.copy()
        ml_columns = []
        
        for col in multi_cols_except_tanilar:
            if col in df_with_ml.columns:
                print(f"   ğŸ“‹ {col} iÃ§in binary Ã¶zellikler oluÅŸturuluyor...")
                df_with_ml, new_cols = create_multilabel_features(
                    df_with_ml, col, top_k=top_k, prefix="ML"
                )
                ml_columns.extend(new_cols)
        
        # TanÄ±lar iÃ§in TF-IDF
        if 'Tanilar' in df_processed.columns:
            print("   ğŸ“ TanÄ±lar iÃ§in TF-IDF Ã¶zellikleri oluÅŸturuluyor...")
            docs = join_tokens(df_processed["Tanilar"])
            tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=5)
            tfidf_features = tfidf.fit_transform(docs)
            tfidf_feature_names = [f"tfidf_{name}" for name in tfidf.get_feature_names_out()]
            print(f"      âœ… {len(tfidf_feature_names)} TF-IDF Ã¶zellik oluÅŸturuldu")
        else:
            tfidf_features = None
            tfidf_feature_names = []
        
        # SÃ¼tunlarÄ± kategorize et
        column_info = prepare_feature_columns(df_with_ml, ml_columns)
    
    # Hedef deÄŸiÅŸkeni ayÄ±r
    if column_info['target'] in df_with_ml.columns:
        y = df_with_ml[column_info['target']].values
        print(f"âœ… Hedef deÄŸiÅŸken ayrÄ±ldÄ±: {len(y)} Ã¶rnek")
    else:
        print(f"âŒ Hedef deÄŸiÅŸken bulunamadÄ±: {column_info['target']}")
        sys.exit(1)
    
    # Ã–zellik sÃ¼tunlarÄ±nÄ± seÃ§
    feature_columns = (column_info['numeric'] + 
                      column_info['count'] + 
                      column_info['categorical'] + 
                      column_info['ml_binary'])
    
    X_raw = df_with_ml[feature_columns]
    print(f"âœ… Ã–zellik sÃ¼tunlarÄ± seÃ§ildi: {len(feature_columns)} sÃ¼tun")
    
    # Pipeline oluÅŸtur ve uygula
    preprocessor = create_feature_pipeline(column_info)
    
    # Sparse matrix desteÄŸi iÃ§in sparse_threshold ayarla
    if text_mode == "tfidf" and tfidf_features is not None:
        preprocessor.sparse_threshold = 1.0
    
    X_base = preprocessor.fit_transform(X_raw)
    
    # TF-IDF features varsa birleÅŸtir
    if text_mode == "tfidf" and tfidf_features is not None:
        print("ğŸ”— TF-IDF Ã¶zelliklerini ana matris ile birleÅŸtiriliyor...")
        X = hstack([X_base, tfidf_features])
        
        # Ã–zellik isimlerini birleÅŸtir
        base_feature_names = get_feature_names(preprocessor, column_info)
        feature_names = base_feature_names + tfidf_feature_names
    else:
        X = X_base
        feature_names = get_feature_names(preprocessor, column_info)
    
    print(f"âœ… Ã–zellik matrisi oluÅŸturuldu: {X.shape}")
    print(f"   ğŸ“Š Boyut: {X.shape[0]} Ã¶rnek Ã— {X.shape[1]} Ã¶zellik")
    print(f"   ğŸ”§ Matrix tipi: {'Sparse' if hasattr(X, 'toarray') else 'Dense'}")
    
    return X, y, feature_names, column_info


def save_features(X: np.ndarray, y: np.ndarray, feature_names: list, 
                 column_info: dict, output_dir: str) -> None:
    """
    Ã–zellik matrisini ve hedef deÄŸiÅŸkeni kaydeder.
    
    Args:
        X (np.ndarray): Ã–zellik matrisi
        y (np.ndarray): Hedef deÄŸiÅŸken
        feature_names (list): Ã–zellik isimleri
        column_info (dict): SÃ¼tun bilgileri
        output_dir (str): Ã‡Ä±ktÄ± dizini
    """
    ensure_directory_exists(output_dir)
    
    # Ã–zellik matrisini kaydet
    if hasattr(X, 'toarray'):  # Sparse matrix
        print("âš ï¸  Sparse matrix CSV olarak kaydediliyor (bÃ¼yÃ¼k dosya olabilir)")
        X_dense = X.toarray()
        X_df = pd.DataFrame(X_dense, columns=feature_names)
    else:
        X_df = pd.DataFrame(X, columns=feature_names)
    
    X_path = os.path.join(output_dir, 'X_model_ready.csv')
    X_df.to_csv(X_path, index=False, encoding='utf-8-sig')
    print(f"âœ… Ã–zellik matrisi kaydedildi: {X_path}")
    
    # Hedef deÄŸiÅŸkeni kaydet
    y_df = pd.DataFrame(y, columns=['TedaviSuresi_num'])
    y_path = os.path.join(output_dir, 'y.csv')
    y_df.to_csv(y_path, index=False, encoding='utf-8-sig')
    print(f"âœ… Hedef deÄŸiÅŸken kaydedildi: {y_path}")
    
    # Ã–zellik bilgilerini kaydet
    feature_info = pd.DataFrame({
        'feature_name': feature_names,
        'feature_type': ['unknown'] * len(feature_names)  # Tip bilgisini gÃ¼ncelle
    })
    
    # Ã–zellik tiplerini gÃ¼ncelle
    for i, name in enumerate(feature_names):
        if name.startswith('num_'):
            feature_info.loc[i, 'feature_type'] = 'numeric_standardized'
        elif name.startswith('count_'):
            feature_info.loc[i, 'feature_type'] = 'count_standardized'
        elif name.startswith('cat_'):
            feature_info.loc[i, 'feature_type'] = 'categorical_onehot'
        elif name.startswith('ML_'):
            feature_info.loc[i, 'feature_type'] = 'multilabel_binary'
    
    feature_info_path = os.path.join(output_dir, 'feature_info.csv')
    feature_info.to_csv(feature_info_path, index=False, encoding='utf-8-sig')
    print(f"âœ… Ã–zellik bilgileri kaydedildi: {feature_info_path}")
    
    # Ã–zet bilgileri kaydet
    summary_info = {
        'total_features': len(feature_names),
        'total_samples': len(y),
        'numeric_features': len([n for n in feature_names if n.startswith('num_')]),
        'count_features': len([n for n in feature_names if n.startswith('count_')]),
        'categorical_features': len([n for n in feature_names if n.startswith('cat_')]),
        'multilabel_features': len([n for n in feature_names if n.startswith('ML_')]),
        'target_mean': float(y.mean()),
        'target_std': float(y.std()),
        'target_min': float(y.min()),
        'target_max': float(y.max())
    }
    
    summary_df = pd.DataFrame([summary_info])
    summary_path = os.path.join(output_dir, 'feature_summary.csv')
    summary_df.to_csv(summary_path, index=False, encoding='utf-8-sig')
    print(f"âœ… Ã–zellik Ã¶zeti kaydedildi: {summary_path}")


def main():
    """Ana fonksiyon - komut satÄ±rÄ± argÃ¼manlarÄ±nÄ± iÅŸler ve Ã¶zellik mÃ¼hendisliÄŸi yapar."""
    parser = argparse.ArgumentParser(
        description="Fiziksel TÄ±p & Rehabilitasyon Veri Analizi - Ã–zellik MÃ¼hendisliÄŸi Script",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ã–rnekler:
  python src/03_build_features.py --input-csv data/processed/clean_minimal.csv --top_k 50
  python src/03_build_features.py --input-csv data/processed/model_ready_minimal.csv --top_k 30

Ã‡Ä±ktÄ±lar:
  - data/processed/X_model_ready.csv: Ã–zellik matrisi
  - data/processed/y.csv: Hedef deÄŸiÅŸken
  - data/processed/feature_info.csv: Ã–zellik bilgileri
  - data/processed/feature_summary.csv: Ã–zellik Ã¶zeti
        """
    )
    
    parser.add_argument(
        '--input-csv',
        required=True,
        help='TemizlenmiÅŸ CSV dosya yolu (Ã¶rn: data/processed/clean_minimal.csv)'
    )
    
    parser.add_argument(
        '--top_k',
        type=int,
        default=50,
        help='Ã‡oklu deÄŸerli alanlar iÃ§in en sÄ±k gÃ¶rÃ¼len kaÃ§ Ã¶ÄŸe (varsayÄ±lan: 50)'
    )
    
    parser.add_argument(
        '--min_freq',
        type=int,
        default=10,
        help='Nadir kategoriler iÃ§in minimum frekans eÅŸiÄŸi (varsayÄ±lan: 10)'
    )
    
    parser.add_argument(
        '--text_mode',
        choices=['mlbin', 'tfidf'],
        default='mlbin',
        help='Metin Ã¶zellik Ã§Ä±karma modu: mlbin (multi-label binary) veya tfidf (varsayÄ±lan: mlbin)'
    )
    
    args = parser.parse_args()
    
    logger.info("ğŸš€ Fiziksel TÄ±p & Rehabilitasyon Ã–zellik MÃ¼hendisliÄŸi Script'i BaÅŸlatÄ±lÄ±yor...")
    logger.info(f"ğŸ“ Input CSV: {args.input_csv}")
    logger.info(f"ğŸ”¢ Top-K: {args.top_k}")
    logger.info(f"ğŸ“Š Min Freq: {args.min_freq}")
    logger.info(f"ğŸ“ Text Mode: {args.text_mode}")
    
    # Ã‡Ä±ktÄ± dizinini hazÄ±rla
    output_dir = 'data/processed'
    ensure_directory_exists(output_dir)
    
    # Veriyi yÃ¼kle
    df = load_data(args.input_csv)
    
    # Ã–zellik matrisini oluÅŸtur
    X, y, feature_names, column_info = build_features(df, top_k=args.top_k, min_freq=args.min_freq, text_mode=args.text_mode)
    
    # SonuÃ§larÄ± kaydet
    logger.info("ğŸ’¾ Ã–zellik matrisi kaydediliyor...")
    save_features(X, y, feature_names, column_info, output_dir)
    
    logger.info("âœ… Ã–zellik mÃ¼hendisliÄŸi tamamlandÄ±!")
    logger.info(f"ğŸ“ Ã‡Ä±ktÄ± dosyalarÄ±: {output_dir}/")
    logger.info("   - X_model_ready.csv: Ã–zellik matrisi")
    logger.info("   - y.csv: Hedef deÄŸiÅŸken")
    logger.info("   - feature_info.csv: Ã–zellik bilgileri")
    logger.info("   - feature_summary.csv: Ã–zellik Ã¶zeti")
    
    logger.info("ğŸ“Š Final Ã–zetler:")
    logger.info(f"   ğŸ¯ Ã–rnekler: {len(y):,}")
    logger.info(f"   ğŸ”§ Ã–zellikler: {len(feature_names):,}")
    logger.info(f"   ğŸ“ˆ Hedef ortalama: {y.mean():.2f}")
    logger.info(f"   ğŸ“‰ Hedef std: {y.std():.2f}")


if __name__ == "__main__":
    main()
