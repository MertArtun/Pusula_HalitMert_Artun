#!/usr/bin/env python3
"""
Fiziksel TÄ±p & Rehabilitasyon Veri Analizi - Ã–zellik MÃ¼hendisliÄŸi

Bu script temizlenmiÅŸ veriyi alÄ±r ve model iÃ§in hazÄ±r Ã¶zellik matrisi oluÅŸturur.
One-Hot Encoding, Ã§oklu deÄŸerli alanlar iÃ§in binary Ã¶zellikler ve 
standardizasyon uygular.

KullanÄ±m:
    python src/03_build_features.py --input-csv data/processed/clean_minimal.csv --top_k 50
"""

import argparse
import os
import sys
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline

# utils modÃ¼lÃ¼nÃ¼ import et
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from utils import (
    create_multilabel_features,
    ensure_directory_exists
)


def load_data(csv_path: str) -> pd.DataFrame:
    """
    CSV dosyasÄ±ndan veriyi yÃ¼kler.
    
    Args:
        csv_path (str): CSV dosya yolu
        
    Returns:
        pd.DataFrame: YÃ¼klenen veri
    """
    try:
        df = pd.read_csv(csv_path)
        print(f"âœ… Veri baÅŸarÄ±yla yÃ¼klendi: {df.shape[0]} satÄ±r, {df.shape[1]} sÃ¼tun")
        return df
    except FileNotFoundError:
        print(f"âŒ Hata: CSV dosyasÄ± bulunamadÄ±: {csv_path}")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ Hata: CSV dosyasÄ± okunurken hata oluÅŸtu: {e}")
        sys.exit(1)


def create_multilabel_features_all(df: pd.DataFrame, top_k: int = 50) -> tuple:
    """
    TÃ¼m Ã§oklu deÄŸerli alanlar iÃ§in binary Ã¶zellikler oluÅŸturur.
    
    Args:
        df (pd.DataFrame): Veri Ã§erÃ§evesi
        top_k (int): Her alan iÃ§in en sÄ±k gÃ¶rÃ¼len kaÃ§ Ã¶ÄŸe
        
    Returns:
        tuple: (Ã¶zellik_eklenmis_df, yeni_kolon_listesi)
    """
    print(f"ğŸ”§ Ã‡oklu deÄŸerli alanlar iÃ§in top-{top_k} binary Ã¶zellikler oluÅŸturuluyor...")
    
    df_features = df.copy()
    all_new_columns = []
    
    multi_value_columns = ['KronikHastalik', 'Alerji', 'Tanilar', 'UygulamaYerleri']
    existing_multi_cols = [col for col in multi_value_columns if col in df.columns]
    
    for col in existing_multi_cols:
        print(f"   ğŸ“‹ {col} iÃ§in binary Ã¶zellikler oluÅŸturuluyor...")
        df_features, new_cols = create_multilabel_features(
            df_features, col, top_k=top_k, prefix="ML"
        )
        all_new_columns.extend(new_cols)
        print(f"      âœ… {len(new_cols)} binary Ã¶zellik eklendi")
    
    print(f"âœ… Toplam {len(all_new_columns)} binary Ã¶zellik oluÅŸturuldu")
    return df_features, all_new_columns


def prepare_feature_columns(df: pd.DataFrame, ml_columns: list) -> dict:
    """
    FarklÄ± veri tiplerindeki sÃ¼tunlarÄ± kategorilere ayÄ±rÄ±r.
    
    Args:
        df (pd.DataFrame): Veri Ã§erÃ§evesi
        ml_columns (list): ML binary sÃ¼tunlarÄ±
        
    Returns:
        dict: SÃ¼tun kategorileri
    """
    print("ğŸ“Š Ã–zellik sÃ¼tunlarÄ± kategorize ediliyor...")
    
    # Hedef deÄŸiÅŸken ve ID sÃ¼tunlarÄ±
    target_column = 'TedaviSuresi_num'
    id_columns = ['HastaNo']
    
    # Kategorik sÃ¼tunlar (One-Hot encoding iÃ§in)
    categorical_columns = [
        'Cinsiyet', 'KanGrubu', 'Uyruk', 'Bolum', 'TedaviAdi'
    ]
    existing_categorical = [col for col in categorical_columns if col in df.columns]
    
    # SayÄ±sal sÃ¼tunlar (standardizasyon iÃ§in)
    numeric_columns = []
    for col in df.select_dtypes(include=[np.number]).columns:
        if (col not in id_columns and 
            col != target_column and 
            not col.endswith('_sayisi') and
            col not in ml_columns):
            numeric_columns.append(col)
    
    # SayÄ± sÃ¼tunlarÄ± (Ã§oklu deÄŸerli alanlarÄ±n sayÄ±larÄ±)
    count_columns = [col for col in df.columns if col.endswith('_sayisi')]
    
    # Atlanacak sÃ¼tunlar
    skip_columns = (id_columns + [target_column] + 
                   ['TedaviSuresi', 'UygulamaSuresi'] +  # Orijinal string sÃ¼tunlarÄ±
                   ['KronikHastalik', 'Alerji', 'Tanilar', 'UygulamaYerleri'])  # Ã‡oklu deÄŸerli orijinal sÃ¼tunlar
    
    column_info = {
        'target': target_column,
        'id': id_columns,
        'categorical': existing_categorical,
        'numeric': numeric_columns,
        'count': count_columns,
        'ml_binary': ml_columns,
        'skip': skip_columns
    }
    
    print(f"   ğŸ¯ Hedef: {target_column}")
    print(f"   ğŸ·ï¸  Kategorik: {len(existing_categorical)} sÃ¼tun")
    print(f"   ğŸ”¢ SayÄ±sal: {len(numeric_columns)} sÃ¼tun")
    print(f"   ğŸ“Š SayÄ±: {len(count_columns)} sÃ¼tun")
    print(f"   ğŸ¤– ML Binary: {len(ml_columns)} sÃ¼tun")
    print(f"   â­ï¸  Atlanan: {len(skip_columns)} sÃ¼tun")
    
    return column_info


def create_feature_pipeline(column_info: dict) -> ColumnTransformer:
    """
    Ã–zellik iÅŸleme pipeline'Ä± oluÅŸturur.
    
    Args:
        column_info (dict): SÃ¼tun bilgileri
        
    Returns:
        ColumnTransformer: Ã–zellik iÅŸleme pipeline'Ä±
    """
    print("ğŸ”§ Ã–zellik iÅŸleme pipeline'Ä± oluÅŸturuluyor...")
    
    transformers = []
    
    # SayÄ±sal sÃ¼tunlar iÃ§in: median imputation + standardization
    if column_info['numeric']:
        numeric_pipeline = Pipeline([
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ])
        transformers.append(('numeric', numeric_pipeline, column_info['numeric']))
        print(f"   ğŸ”¢ SayÄ±sal pipeline: median imputation + standardization")
    
    # SayÄ± sÃ¼tunlarÄ± iÃ§in: 0 ile doldur + standardization
    if column_info['count']:
        count_pipeline = Pipeline([
            ('imputer', SimpleImputer(strategy='constant', fill_value=0)),
            ('scaler', StandardScaler())
        ])
        transformers.append(('count', count_pipeline, column_info['count']))
        print(f"   ğŸ“Š SayÄ± pipeline: 0 ile doldur + standardization")
    
    # Kategorik sÃ¼tunlar iÃ§in: most frequent + one-hot encoding
    if column_info['categorical']:
        categorical_pipeline = Pipeline([
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))
        ])
        transformers.append(('categorical', categorical_pipeline, column_info['categorical']))
        print(f"   ğŸ·ï¸  Kategorik pipeline: most frequent + one-hot encoding")
    
    # ML binary sÃ¼tunlarÄ± iÃ§in: hiÃ§bir iÅŸlem yapmaz (zaten 0/1)
    if column_info['ml_binary']:
        from sklearn.preprocessing import FunctionTransformer
        ml_pipeline = FunctionTransformer(validate=False)
        transformers.append(('ml_binary', ml_pipeline, column_info['ml_binary']))
        print(f"   ğŸ¤– ML Binary: hiÃ§bir iÅŸlem uygulanmaz")
    
    # ColumnTransformer oluÅŸtur
    preprocessor = ColumnTransformer(
        transformers=transformers,
        remainder='drop'  # DiÄŸer sÃ¼tunlarÄ± at
    )
    
    return preprocessor


def get_feature_names(preprocessor: ColumnTransformer, column_info: dict) -> list:
    """
    Ä°ÅŸlenmiÅŸ Ã¶zellik isimlerini oluÅŸturur.
    
    Args:
        preprocessor (ColumnTransformer): Fit edilmiÅŸ preprocessor
        column_info (dict): SÃ¼tun bilgileri
        
    Returns:
        list: Ã–zellik isimleri
    """
    feature_names = []
    
    # SayÄ±sal sÃ¼tun isimleri
    if column_info['numeric']:
        feature_names.extend([f"num_{col}" for col in column_info['numeric']])
    
    # SayÄ± sÃ¼tun isimleri
    if column_info['count']:
        feature_names.extend([f"count_{col}" for col in column_info['count']])
    
    # Kategorik sÃ¼tun isimleri (One-Hot encoding sonrasÄ±)
    if column_info['categorical']:
        # One-Hot encoder'dan Ã¶zellik isimlerini al
        categorical_transformer = preprocessor.named_transformers_['categorical']
        onehot_encoder = categorical_transformer.named_steps['onehot']
        
        cat_feature_names = []
        for i, col in enumerate(column_info['categorical']):
            categories = onehot_encoder.categories_[i]
            # drop='first' kullandÄ±ÄŸÄ±mÄ±z iÃ§in ilk kategoriyi atla
            for cat in categories[1:]:
                cat_feature_names.append(f"cat_{col}_{cat}")
        
        feature_names.extend(cat_feature_names)
    
    # ML binary sÃ¼tun isimleri
    if column_info['ml_binary']:
        feature_names.extend(column_info['ml_binary'])
    
    return feature_names


def build_features(df: pd.DataFrame, top_k: int = 50) -> tuple:
    """
    Ã–zellik matrisini oluÅŸturur.
    
    Args:
        df (pd.DataFrame): TemizlenmiÅŸ veri
        top_k (int): Ã‡oklu deÄŸerli alanlar iÃ§in top-K
        
    Returns:
        tuple: (X, y, feature_names, column_info)
    """
    print("ğŸ—ï¸  Ã–zellik matrisi oluÅŸturuluyor...")
    
    # Ã‡oklu deÄŸerli alanlar iÃ§in binary Ã¶zellikler oluÅŸtur
    df_with_ml, ml_columns = create_multilabel_features_all(df, top_k)
    
    # SÃ¼tunlarÄ± kategorize et
    column_info = prepare_feature_columns(df_with_ml, ml_columns)
    
    # Hedef deÄŸiÅŸkeni ayÄ±r
    if column_info['target'] in df_with_ml.columns:
        y = df_with_ml[column_info['target']].values
        print(f"âœ… Hedef deÄŸiÅŸken ayrÄ±ldÄ±: {len(y)} Ã¶rnek")
    else:
        print(f"âŒ Hedef deÄŸiÅŸken bulunamadÄ±: {column_info['target']}")
        sys.exit(1)
    
    # Ã–zellik sÃ¼tunlarÄ±nÄ± seÃ§
    feature_columns = (column_info['numeric'] + 
                      column_info['count'] + 
                      column_info['categorical'] + 
                      column_info['ml_binary'])
    
    X_raw = df_with_ml[feature_columns]
    print(f"âœ… Ã–zellik sÃ¼tunlarÄ± seÃ§ildi: {len(feature_columns)} sÃ¼tun")
    
    # Pipeline oluÅŸtur ve uygula
    preprocessor = create_feature_pipeline(column_info)
    X = preprocessor.fit_transform(X_raw)
    
    # Ã–zellik isimlerini oluÅŸtur
    feature_names = get_feature_names(preprocessor, column_info)
    
    print(f"âœ… Ã–zellik matrisi oluÅŸturuldu: {X.shape}")
    print(f"   ğŸ“Š Boyut: {X.shape[0]} Ã¶rnek Ã— {X.shape[1]} Ã¶zellik")
    
    return X, y, feature_names, column_info


def save_features(X: np.ndarray, y: np.ndarray, feature_names: list, 
                 column_info: dict, output_dir: str) -> None:
    """
    Ã–zellik matrisini ve hedef deÄŸiÅŸkeni kaydeder.
    
    Args:
        X (np.ndarray): Ã–zellik matrisi
        y (np.ndarray): Hedef deÄŸiÅŸken
        feature_names (list): Ã–zellik isimleri
        column_info (dict): SÃ¼tun bilgileri
        output_dir (str): Ã‡Ä±ktÄ± dizini
    """
    ensure_directory_exists(output_dir)
    
    # Ã–zellik matrisini kaydet
    X_df = pd.DataFrame(X, columns=feature_names)
    X_path = os.path.join(output_dir, 'X_model_ready.csv')
    X_df.to_csv(X_path, index=False, encoding='utf-8-sig')
    print(f"âœ… Ã–zellik matrisi kaydedildi: {X_path}")
    
    # Hedef deÄŸiÅŸkeni kaydet
    y_df = pd.DataFrame(y, columns=['TedaviSuresi_num'])
    y_path = os.path.join(output_dir, 'y.csv')
    y_df.to_csv(y_path, index=False, encoding='utf-8-sig')
    print(f"âœ… Hedef deÄŸiÅŸken kaydedildi: {y_path}")
    
    # Ã–zellik bilgilerini kaydet
    feature_info = pd.DataFrame({
        'feature_name': feature_names,
        'feature_type': ['unknown'] * len(feature_names)  # Tip bilgisini gÃ¼ncelle
    })
    
    # Ã–zellik tiplerini gÃ¼ncelle
    for i, name in enumerate(feature_names):
        if name.startswith('num_'):
            feature_info.loc[i, 'feature_type'] = 'numeric_standardized'
        elif name.startswith('count_'):
            feature_info.loc[i, 'feature_type'] = 'count_standardized'
        elif name.startswith('cat_'):
            feature_info.loc[i, 'feature_type'] = 'categorical_onehot'
        elif name.startswith('ML_'):
            feature_info.loc[i, 'feature_type'] = 'multilabel_binary'
    
    feature_info_path = os.path.join(output_dir, 'feature_info.csv')
    feature_info.to_csv(feature_info_path, index=False, encoding='utf-8-sig')
    print(f"âœ… Ã–zellik bilgileri kaydedildi: {feature_info_path}")
    
    # Ã–zet bilgileri kaydet
    summary_info = {
        'total_features': len(feature_names),
        'total_samples': len(y),
        'numeric_features': len([n for n in feature_names if n.startswith('num_')]),
        'count_features': len([n for n in feature_names if n.startswith('count_')]),
        'categorical_features': len([n for n in feature_names if n.startswith('cat_')]),
        'multilabel_features': len([n for n in feature_names if n.startswith('ML_')]),
        'target_mean': float(y.mean()),
        'target_std': float(y.std()),
        'target_min': float(y.min()),
        'target_max': float(y.max())
    }
    
    summary_df = pd.DataFrame([summary_info])
    summary_path = os.path.join(output_dir, 'feature_summary.csv')
    summary_df.to_csv(summary_path, index=False, encoding='utf-8-sig')
    print(f"âœ… Ã–zellik Ã¶zeti kaydedildi: {summary_path}")


def main():
    """Ana fonksiyon - komut satÄ±rÄ± argÃ¼manlarÄ±nÄ± iÅŸler ve Ã¶zellik mÃ¼hendisliÄŸi yapar."""
    parser = argparse.ArgumentParser(
        description="Fiziksel TÄ±p & Rehabilitasyon Veri Analizi - Ã–zellik MÃ¼hendisliÄŸi Script",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ã–rnekler:
  python src/03_build_features.py --input-csv data/processed/clean_minimal.csv --top_k 50
  python src/03_build_features.py --input-csv data/processed/model_ready_minimal.csv --top_k 30

Ã‡Ä±ktÄ±lar:
  - data/processed/X_model_ready.csv: Ã–zellik matrisi
  - data/processed/y.csv: Hedef deÄŸiÅŸken
  - data/processed/feature_info.csv: Ã–zellik bilgileri
  - data/processed/feature_summary.csv: Ã–zellik Ã¶zeti
        """
    )
    
    parser.add_argument(
        '--input-csv',
        required=True,
        help='TemizlenmiÅŸ CSV dosya yolu (Ã¶rn: data/processed/clean_minimal.csv)'
    )
    
    parser.add_argument(
        '--top_k',
        type=int,
        default=50,
        help='Ã‡oklu deÄŸerli alanlar iÃ§in en sÄ±k gÃ¶rÃ¼len kaÃ§ Ã¶ÄŸe (varsayÄ±lan: 50)'
    )
    
    args = parser.parse_args()
    
    print("ğŸš€ Fiziksel TÄ±p & Rehabilitasyon Ã–zellik MÃ¼hendisliÄŸi Script'i BaÅŸlatÄ±lÄ±yor...")
    print(f"ğŸ“ Input CSV: {args.input_csv}")
    print(f"ğŸ”¢ Top-K: {args.top_k}")
    
    # Ã‡Ä±ktÄ± dizinini hazÄ±rla
    output_dir = 'data/processed'
    ensure_directory_exists(output_dir)
    
    # Veriyi yÃ¼kle
    df = load_data(args.input_csv)
    
    # Ã–zellik matrisini oluÅŸtur
    X, y, feature_names, column_info = build_features(df, top_k=args.top_k)
    
    # SonuÃ§larÄ± kaydet
    print(f"\nğŸ’¾ Ã–zellik matrisi kaydediliyor...")
    save_features(X, y, feature_names, column_info, output_dir)
    
    print(f"\nâœ… Ã–zellik mÃ¼hendisliÄŸi tamamlandÄ±!")
    print(f"ğŸ“ Ã‡Ä±ktÄ± dosyalarÄ±: {output_dir}/")
    print("   - X_model_ready.csv: Ã–zellik matrisi")
    print("   - y.csv: Hedef deÄŸiÅŸken")
    print("   - feature_info.csv: Ã–zellik bilgileri")
    print("   - feature_summary.csv: Ã–zellik Ã¶zeti")
    
    print(f"\nğŸ“Š Final Ã–zetler:")
    print(f"   ğŸ¯ Ã–rnekler: {len(y):,}")
    print(f"   ğŸ”§ Ã–zellikler: {len(feature_names):,}")
    print(f"   ğŸ“ˆ Hedef ortalama: {y.mean():.2f}")
    print(f"   ğŸ“‰ Hedef std: {y.std():.2f}")


if __name__ == "__main__":
    main()
