#!/usr/bin/env python3
"""
Fiziksel Tƒ±p & Rehabilitasyon Veri Analizi - √ñzellik M√ºhendisliƒüi

Bu script temizlenmi≈ü veriyi alƒ±r ve model i√ßin hazƒ±r √∂zellik matrisi olu≈üturur.
One-Hot Encoding, √ßoklu deƒüerli alanlar i√ßin binary √∂zellikler ve 
standardizasyon uygular.

Kullanƒ±m:
    python src/03_build_features.py --input-csv data/processed/clean_minimal.csv --top_k 50
"""

import argparse
import os
import sys
import pandas as pd
import numpy as np
from pathlib import Path
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack

# utils mod√ºl√ºn√º import et
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from utils import (
    create_multilabel_features,
    ensure_directory_exists,
    normalize_text_token,
    split_list
)
from common_logging import features_logger as logger


def load_data(csv_path: str) -> pd.DataFrame:
    """
    CSV dosyasƒ±ndan veriyi y√ºkler.
    
    Args:
        csv_path (str): CSV dosya yolu
        
    Returns:
        pd.DataFrame: Y√ºklenen veri
    """
    try:
        df = pd.read_csv(csv_path)
        logger.info(f"‚úÖ Veri ba≈üarƒ±yla y√ºklendi: {df.shape[0]} satƒ±r, {df.shape[1]} s√ºtun")
        return df
    except FileNotFoundError:
        logger.error(f"‚ùå Hata: CSV dosyasƒ± bulunamadƒ±: {csv_path}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"‚ùå Hata: CSV dosyasƒ± okunurken hata olu≈ütu: {e}")
        sys.exit(1)


def collapse_rare(series: pd.Series, min_freq: int = 10, other_label: str = "Diger") -> pd.Series:
    """
    Nadir kategorileri belirtilen etiket altƒ±nda birle≈ütirir.
    
    Args:
        series (pd.Series): Kategorik seri
        min_freq (int): Minimum frekans e≈üiƒüi
        other_label (str): Nadir kategoriler i√ßin etiket
        
    Returns:
        pd.Series: Nadir kategoriler birle≈ütirilmi≈ü seri
    """
    vc = series.value_counts(dropna=False)
    keep = set(vc[vc >= min_freq].index)
    return series.apply(lambda x: x if x in keep else other_label)


def join_tokens(series: pd.Series) -> list:
    """
    √áoklu deƒüerli seriyi normalize edilmi≈ü token'larla birle≈ütirir.
    
    Args:
        series (pd.Series): √áoklu deƒüerli seri
        
    Returns:
        list: Her satƒ±r i√ßin normalize edilmi≈ü token'lar birle≈ütirilmi≈ü string listesi
    """
    docs = []
    for v in series.fillna(""):
        toks = [normalize_text_token(t) for t in split_list(v)]
        docs.append(" ".join(toks))
    return docs


def create_multilabel_features_all(df: pd.DataFrame, top_k: int = 50) -> tuple:
    """
    T√ºm √ßoklu deƒüerli alanlar i√ßin binary √∂zellikler olu≈üturur.
    
    Args:
        df (pd.DataFrame): Veri √ßer√ßevesi
        top_k (int): Her alan i√ßin en sƒ±k g√∂r√ºlen ka√ß √∂ƒüe
        
    Returns:
        tuple: (√∂zellik_eklenmis_df, yeni_kolon_listesi)
    """
    print(f"üîß √áoklu deƒüerli alanlar i√ßin top-{top_k} binary √∂zellikler olu≈üturuluyor...")
    
    df_features = df.copy()
    all_new_columns = []
    
    multi_value_columns = ['KronikHastalik', 'Alerji', 'Tanilar', 'UygulamaYerleri']
    existing_multi_cols = [col for col in multi_value_columns if col in df.columns]
    
    for col in existing_multi_cols:
        print(f"   üìã {col} i√ßin binary √∂zellikler olu≈üturuluyor...")
        df_features, new_cols = create_multilabel_features(
            df_features, col, top_k=top_k, prefix="ML"
        )
        all_new_columns.extend(new_cols)
        print(f"      ‚úÖ {len(new_cols)} binary √∂zellik eklendi")
    
    print(f"‚úÖ Toplam {len(all_new_columns)} binary √∂zellik olu≈üturuldu")
    return df_features, all_new_columns


def prepare_feature_columns(df: pd.DataFrame, ml_columns: list) -> dict:
    """
    Farklƒ± veri tiplerindeki s√ºtunlarƒ± kategorilere ayƒ±rƒ±r.
    
    Args:
        df (pd.DataFrame): Veri √ßer√ßevesi
        ml_columns (list): ML binary s√ºtunlarƒ±
        
    Returns:
        dict: S√ºtun kategorileri
    """
    print("üìä √ñzellik s√ºtunlarƒ± kategorize ediliyor...")
    
    # Hedef deƒüi≈üken ve ID s√ºtunlarƒ±
    target_column = 'TedaviSuresi_num'
    id_columns = ['HastaNo']
    
    # Kategorik s√ºtunlar (One-Hot encoding i√ßin)
    categorical_columns = [
        'Cinsiyet', 'KanGrubu', 'Uyruk', 'Bolum', 'TedaviAdi'
    ]
    existing_categorical = [col for col in categorical_columns if col in df.columns]
    
    # Sayƒ±sal s√ºtunlar (standardizasyon i√ßin)
    numeric_columns = []
    for col in df.select_dtypes(include=[np.number]).columns:
        if (col not in id_columns and 
            col != target_column and 
            not col.endswith('_sayisi') and
            col not in ml_columns):
            numeric_columns.append(col)
    
    # Sayƒ± s√ºtunlarƒ± (√ßoklu deƒüerli alanlarƒ±n sayƒ±larƒ±)
    count_columns = [col for col in df.columns if col.endswith('_sayisi')]
    
    # Atlanacak s√ºtunlar
    skip_columns = (id_columns + [target_column] + 
                   ['TedaviSuresi', 'UygulamaSuresi'] +  # Orijinal string s√ºtunlarƒ±
                   ['KronikHastalik', 'Alerji', 'Tanilar', 'UygulamaYerleri'])  # √áoklu deƒüerli orijinal s√ºtunlar
    
    column_info = {
        'target': target_column,
        'id': id_columns,
        'categorical': existing_categorical,
        'numeric': numeric_columns,
        'count': count_columns,
        'ml_binary': ml_columns,
        'skip': skip_columns
    }
    
    print(f"   üéØ Hedef: {target_column}")
    print(f"   üè∑Ô∏è  Kategorik: {len(existing_categorical)} s√ºtun")
    print(f"   üî¢ Sayƒ±sal: {len(numeric_columns)} s√ºtun")
    print(f"   üìä Sayƒ±: {len(count_columns)} s√ºtun")
    print(f"   ü§ñ ML Binary: {len(ml_columns)} s√ºtun")
    print(f"   ‚è≠Ô∏è  Atlanan: {len(skip_columns)} s√ºtun")
    
    return column_info


def create_feature_pipeline(column_info: dict) -> ColumnTransformer:
    """
    √ñzellik i≈üleme pipeline'ƒ± olu≈üturur.
    
    Args:
        column_info (dict): S√ºtun bilgileri
        
    Returns:
        ColumnTransformer: √ñzellik i≈üleme pipeline'ƒ±
    """
    print("üîß √ñzellik i≈üleme pipeline'ƒ± olu≈üturuluyor...")
    
    transformers = []
    
    # Sayƒ±sal s√ºtunlar i√ßin: median imputation + standardization
    if column_info['numeric']:
        numeric_pipeline = Pipeline([
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ])
        transformers.append(('numeric', numeric_pipeline, column_info['numeric']))
        print(f"   üî¢ Sayƒ±sal pipeline: median imputation + standardization")
    
    # Sayƒ± s√ºtunlarƒ± i√ßin: 0 ile doldur + standardization
    if column_info['count']:
        count_pipeline = Pipeline([
            ('imputer', SimpleImputer(strategy='constant', fill_value=0)),
            ('scaler', StandardScaler())
        ])
        transformers.append(('count', count_pipeline, column_info['count']))
        print(f"   üìä Sayƒ± pipeline: 0 ile doldur + standardization")
    
    # Kategorik s√ºtunlar i√ßin: most frequent + one-hot encoding
    if column_info['categorical']:
        categorical_pipeline = Pipeline([
            ('imputer', SimpleImputer(strategy='most_frequent')),
            ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))
        ])
        transformers.append(('categorical', categorical_pipeline, column_info['categorical']))
        print(f"   üè∑Ô∏è  Kategorik pipeline: most frequent + one-hot encoding")
    
    # ML binary s√ºtunlarƒ± i√ßin: hi√ßbir i≈ülem yapmaz (zaten 0/1)
    if column_info['ml_binary']:
        from sklearn.preprocessing import FunctionTransformer
        ml_pipeline = FunctionTransformer(validate=False)
        transformers.append(('ml_binary', ml_pipeline, column_info['ml_binary']))
        print(f"   ü§ñ ML Binary: hi√ßbir i≈ülem uygulanmaz")
    
    # ColumnTransformer olu≈ütur
    preprocessor = ColumnTransformer(
        transformers=transformers,
        remainder='drop'  # Diƒüer s√ºtunlarƒ± at
    )
    
    return preprocessor


def get_feature_names(preprocessor: ColumnTransformer, column_info: dict) -> list:
    """
    ƒ∞≈ülenmi≈ü √∂zellik isimlerini olu≈üturur.
    
    Args:
        preprocessor (ColumnTransformer): Fit edilmi≈ü preprocessor
        column_info (dict): S√ºtun bilgileri
        
    Returns:
        list: √ñzellik isimleri
    """
    feature_names = []
    
    # Sayƒ±sal s√ºtun isimleri
    if column_info['numeric']:
        feature_names.extend([f"num_{col}" for col in column_info['numeric']])
    
    # Sayƒ± s√ºtun isimleri
    if column_info['count']:
        feature_names.extend([f"count_{col}" for col in column_info['count']])
    
    # Kategorik s√ºtun isimleri (One-Hot encoding sonrasƒ±)
    if column_info['categorical']:
        # One-Hot encoder'dan √∂zellik isimlerini al
        categorical_transformer = preprocessor.named_transformers_['categorical']
        onehot_encoder = categorical_transformer.named_steps['onehot']
        
        cat_feature_names = []
        for i, col in enumerate(column_info['categorical']):
            categories = onehot_encoder.categories_[i]
            # drop='first' kullandƒ±ƒüƒ±mƒ±z i√ßin ilk kategoriyi atla
            for cat in categories[1:]:
                cat_feature_names.append(f"cat_{col}_{cat}")
        
        feature_names.extend(cat_feature_names)
    
    # ML binary s√ºtun isimleri
    if column_info['ml_binary']:
        feature_names.extend(column_info['ml_binary'])
    
    return feature_names


def build_features(df: pd.DataFrame, top_k: int = 50, min_freq: int = 10, text_mode: str = "mlbin") -> tuple:
    """
    √ñzellik matrisini olu≈üturur.
    
    Args:
        df (pd.DataFrame): Temizlenmi≈ü veri
        top_k (int): √áoklu deƒüerli alanlar i√ßin top-K
        min_freq (int): Nadir kategoriler i√ßin minimum frekans
        text_mode (str): "mlbin" veya "tfidf"
        
    Returns:
        tuple: (X, y, feature_names, column_info)
    """
    print("üèóÔ∏è  √ñzellik matrisi olu≈üturuluyor...")
    
    df_processed = df.copy()
    
    # Nadir kategorileri birle≈ütir
    print(f"üîß Nadir kategoriler birle≈ütiriliyor (min_freq={min_freq})...")
    base_cat = ["Cinsiyet", "KanGrubu", "Uyruk", "Bolum", "TedaviAdi"]
    for c in base_cat:
        if c in df_processed.columns:
            original_unique = df_processed[c].nunique()
            df_processed[c] = collapse_rare(df_processed[c], min_freq=min_freq)
            new_unique = df_processed[c].nunique()
            print(f"   üìä {c}: {original_unique} ‚Üí {new_unique} kategori")
    
    # Text mode'a g√∂re √∂zellik √ßƒ±karma
    if text_mode == "mlbin":
        print("üîß Multi-label binary √∂zellikler olu≈üturuluyor...")
        # √áoklu deƒüerli alanlar i√ßin binary √∂zellikler olu≈ütur (normalize edilmi≈ü)
        df_with_ml, ml_columns = create_multilabel_features_all(df_processed, top_k)
        
        # S√ºtunlarƒ± kategorize et
        column_info = prepare_feature_columns(df_with_ml, ml_columns)
        
        # TF-IDF kullanmayacaƒüƒ±z
        tfidf_features = None
        tfidf_feature_names = []
        
    elif text_mode == "tfidf":
        print("üîß TF-IDF √∂zellikleri olu≈üturuluyor (sadece Tanƒ±lar i√ßin)...")
        # Sadece diƒüer √ßoklu deƒüerli alanlar i√ßin ML binary
        multi_cols_except_tanilar = ['KronikHastalik', 'Alerji', 'UygulamaYerleri']
        df_with_ml = df_processed.copy()
        ml_columns = []
        
        for col in multi_cols_except_tanilar:
            if col in df_with_ml.columns:
                print(f"   üìã {col} i√ßin binary √∂zellikler olu≈üturuluyor...")
                df_with_ml, new_cols = create_multilabel_features(
                    df_with_ml, col, top_k=top_k, prefix="ML"
                )
                ml_columns.extend(new_cols)
        
        # Tanƒ±lar i√ßin TF-IDF
        if 'Tanilar' in df_processed.columns:
            print("   üìù Tanƒ±lar i√ßin TF-IDF √∂zellikleri olu≈üturuluyor...")
            docs = join_tokens(df_processed["Tanilar"])
            tfidf = TfidfVectorizer(ngram_range=(1,2), min_df=5)
            tfidf_features = tfidf.fit_transform(docs)
            tfidf_feature_names = [f"tfidf_{name}" for name in tfidf.get_feature_names_out()]
            print(f"      ‚úÖ {len(tfidf_feature_names)} TF-IDF √∂zellik olu≈üturuldu")
        else:
            tfidf_features = None
            tfidf_feature_names = []
        
        # S√ºtunlarƒ± kategorize et
        column_info = prepare_feature_columns(df_with_ml, ml_columns)
    
    # Hedef deƒüi≈ükeni ayƒ±r
    if column_info['target'] in df_with_ml.columns:
        y = df_with_ml[column_info['target']].values
        print(f"‚úÖ Hedef deƒüi≈üken ayrƒ±ldƒ±: {len(y)} √∂rnek")
    else:
        print(f"‚ùå Hedef deƒüi≈üken bulunamadƒ±: {column_info['target']}")
        sys.exit(1)
    
    # √ñzellik s√ºtunlarƒ±nƒ± se√ß
    feature_columns = (column_info['numeric'] + 
                      column_info['count'] + 
                      column_info['categorical'] + 
                      column_info['ml_binary'])
    
    X_raw = df_with_ml[feature_columns]
    print(f"‚úÖ √ñzellik s√ºtunlarƒ± se√ßildi: {len(feature_columns)} s√ºtun")
    
    # Pipeline olu≈ütur ve uygula
    preprocessor = create_feature_pipeline(column_info)
    
    # Sparse matrix desteƒüi i√ßin sparse_threshold ayarla
    if text_mode == "tfidf" and tfidf_features is not None:
        preprocessor.sparse_threshold = 1.0
    
    X_base = preprocessor.fit_transform(X_raw)
    
    # TF-IDF features varsa birle≈ütir
    if text_mode == "tfidf" and tfidf_features is not None:
        print("üîó TF-IDF √∂zelliklerini ana matris ile birle≈ütiriliyor...")
        X = hstack([X_base, tfidf_features])
        
        # √ñzellik isimlerini birle≈ütir
        base_feature_names = get_feature_names(preprocessor, column_info)
        feature_names = base_feature_names + tfidf_feature_names
    else:
        X = X_base
        feature_names = get_feature_names(preprocessor, column_info)
    
    print(f"‚úÖ √ñzellik matrisi olu≈üturuldu: {X.shape}")
    print(f"   üìä Boyut: {X.shape[0]} √∂rnek √ó {X.shape[1]} √∂zellik")
    print(f"   üîß Matrix tipi: {'Sparse' if hasattr(X, 'toarray') else 'Dense'}")
    
    return X, y, feature_names, column_info


def save_features(X: np.ndarray, y: np.ndarray, feature_names: list, 
                 column_info: dict, output_dir: str) -> None:
    """
    √ñzellik matrisini ve hedef deƒüi≈ükeni kaydeder.
    
    Args:
        X (np.ndarray): √ñzellik matrisi
        y (np.ndarray): Hedef deƒüi≈üken
        feature_names (list): √ñzellik isimleri
        column_info (dict): S√ºtun bilgileri
        output_dir (str): √áƒ±ktƒ± dizini
    """
    ensure_directory_exists(output_dir)
    
    # √ñzellik matrisini kaydet
    if hasattr(X, 'toarray'):  # Sparse matrix
        print("‚ö†Ô∏è  Sparse matrix CSV olarak kaydediliyor (b√ºy√ºk dosya olabilir)")
        X_dense = X.toarray()
        X_df = pd.DataFrame(X_dense, columns=feature_names)
    else:
        X_df = pd.DataFrame(X, columns=feature_names)
    
    X_path = os.path.join(output_dir, 'X_model_ready.csv')
    X_df.to_csv(X_path, index=False, encoding='utf-8-sig')
    print(f"‚úÖ √ñzellik matrisi kaydedildi: {X_path}")
    
    # Hedef deƒüi≈ükeni kaydet
    y_df = pd.DataFrame(y, columns=['TedaviSuresi_num'])
    y_path = os.path.join(output_dir, 'y.csv')
    y_df.to_csv(y_path, index=False, encoding='utf-8-sig')
    print(f"‚úÖ Hedef deƒüi≈üken kaydedildi: {y_path}")
    
    # √ñzellik bilgilerini kaydet
    feature_info = pd.DataFrame({
        'feature_name': feature_names,
        'feature_type': ['unknown'] * len(feature_names)  # Tip bilgisini g√ºncelle
    })
    
    # √ñzellik tiplerini g√ºncelle
    for i, name in enumerate(feature_names):
        if name.startswith('num_'):
            feature_info.loc[i, 'feature_type'] = 'numeric_standardized'
        elif name.startswith('count_'):
            feature_info.loc[i, 'feature_type'] = 'count_standardized'
        elif name.startswith('cat_'):
            feature_info.loc[i, 'feature_type'] = 'categorical_onehot'
        elif name.startswith('ML_'):
            feature_info.loc[i, 'feature_type'] = 'multilabel_binary'
    
    feature_info_path = os.path.join(output_dir, 'feature_info.csv')
    feature_info.to_csv(feature_info_path, index=False, encoding='utf-8-sig')
    print(f"‚úÖ √ñzellik bilgileri kaydedildi: {feature_info_path}")
    
    # √ñzet bilgileri kaydet
    summary_info = {
        'total_features': len(feature_names),
        'total_samples': len(y),
        'numeric_features': len([n for n in feature_names if n.startswith('num_')]),
        'count_features': len([n for n in feature_names if n.startswith('count_')]),
        'categorical_features': len([n for n in feature_names if n.startswith('cat_')]),
        'multilabel_features': len([n for n in feature_names if n.startswith('ML_')]),
        'target_mean': float(y.mean()),
        'target_std': float(y.std()),
        'target_min': float(y.min()),
        'target_max': float(y.max())
    }
    
    summary_df = pd.DataFrame([summary_info])
    summary_path = os.path.join(output_dir, 'feature_summary.csv')
    summary_df.to_csv(summary_path, index=False, encoding='utf-8-sig')
    print(f"‚úÖ √ñzellik √∂zeti kaydedildi: {summary_path}")


def main():
    """Ana fonksiyon - komut satƒ±rƒ± arg√ºmanlarƒ±nƒ± i≈üler ve √∂zellik m√ºhendisliƒüi yapar."""
    parser = argparse.ArgumentParser(
        description="Fiziksel Tƒ±p & Rehabilitasyon Veri Analizi - √ñzellik M√ºhendisliƒüi Script",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
√ñrnekler:
  python src/03_build_features.py --input-csv data/processed/clean_minimal.csv --top_k 50
  python src/03_build_features.py --input-csv data/processed/model_ready_minimal.csv --top_k 30

√áƒ±ktƒ±lar:
  - data/processed/X_model_ready.csv: √ñzellik matrisi
  - data/processed/y.csv: Hedef deƒüi≈üken
  - data/processed/feature_info.csv: √ñzellik bilgileri
  - data/processed/feature_summary.csv: √ñzellik √∂zeti
        """
    )
    
    parser.add_argument(
        '--input-csv',
        required=True,
        help='Temizlenmi≈ü CSV dosya yolu (√∂rn: data/processed/clean_minimal.csv)'
    )
    
    parser.add_argument(
        '--top_k',
        type=int,
        default=50,
        help='√áoklu deƒüerli alanlar i√ßin en sƒ±k g√∂r√ºlen ka√ß √∂ƒüe (varsayƒ±lan: 50)'
    )
    
    parser.add_argument(
        '--min_freq',
        type=int,
        default=10,
        help='Nadir kategoriler i√ßin minimum frekans e≈üiƒüi (varsayƒ±lan: 10)'
    )
    
    parser.add_argument(
        '--text_mode',
        choices=['mlbin', 'tfidf'],
        default='mlbin',
        help='Metin √∂zellik √ßƒ±karma modu: mlbin (multi-label binary) veya tfidf (varsayƒ±lan: mlbin)'
    )
    
    args = parser.parse_args()
    
    logger.info("üöÄ Fiziksel Tƒ±p & Rehabilitasyon √ñzellik M√ºhendisliƒüi Script'i Ba≈ülatƒ±lƒ±yor...")
    logger.info(f"üìÅ Input CSV: {args.input_csv}")
    logger.info(f"üî¢ Top-K: {args.top_k}")
    logger.info(f"üìä Min Freq: {args.min_freq}")
    logger.info(f"üìù Text Mode: {args.text_mode}")
    
    # √áƒ±ktƒ± dizinini hazƒ±rla
    output_dir = 'data/processed'
    ensure_directory_exists(output_dir)
    
    # Veriyi y√ºkle
    df = load_data(args.input_csv)
    
    # √ñzellik matrisini olu≈ütur
    X, y, feature_names, column_info = build_features(df, top_k=args.top_k, min_freq=args.min_freq, text_mode=args.text_mode)
    
    # Sonu√ßlarƒ± kaydet
    logger.info("üíæ √ñzellik matrisi kaydediliyor...")
    save_features(X, y, feature_names, column_info, output_dir)
    
    logger.info("‚úÖ √ñzellik m√ºhendisliƒüi tamamlandƒ±!")
    logger.info(f"üìÅ √áƒ±ktƒ± dosyalarƒ±: {output_dir}/")
    logger.info("   - X_model_ready.csv: √ñzellik matrisi")
    logger.info("   - y.csv: Hedef deƒüi≈üken")
    logger.info("   - feature_info.csv: √ñzellik bilgileri")
    logger.info("   - feature_summary.csv: √ñzellik √∂zeti")
    
    logger.info("üìä Final √ñzetler:")
    logger.info(f"   üéØ √ñrnekler: {len(y):,}")
    logger.info(f"   üîß √ñzellikler: {len(feature_names):,}")
    logger.info(f"   üìà Hedef ortalama: {y.mean():.2f}")
    logger.info(f"   üìâ Hedef std: {y.std():.2f}")


if __name__ == "__main__":
    main()
